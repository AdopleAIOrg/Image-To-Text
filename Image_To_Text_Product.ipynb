{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN1t611jZ19/c4cd8oyHa0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdopleAIOrg/Image-To-Text/blob/main/Image_To_Text_Product.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G7AMUlgyNRT"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "from PIL import Image\n",
        "\n",
        "class ImageToText:\n",
        "\n",
        "    def __init__(self):\n",
        "      \"\"\"\n",
        "      Initializes the ImageToText class.\n",
        "      \"\"\"\n",
        "\n",
        "      self.model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "      self.feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "      self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.model.to(self.device)\n",
        "\n",
        "      max_length = 16\n",
        "      num_beams = 4\n",
        "      self.gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "    def _predict_step(self, image_paths: list) -> list:\n",
        "\n",
        "        \"\"\"\n",
        "        Generates captions for the given list of image paths.\n",
        "\n",
        "        Args:\n",
        "            image_paths (list): List of paths to the images.\n",
        "\n",
        "        Returns:\n",
        "            list: List of generated captions for the images.\n",
        "        \"\"\"\n",
        "        images = []\n",
        "        for image_path in image_paths:\n",
        "            i_image = Image.open(image_path)\n",
        "            if i_image.mode != \"RGB\":\n",
        "                i_image = i_image.convert(mode=\"RGB\")\n",
        "\n",
        "            images.append(i_image)\n",
        "\n",
        "        pixel_values = self.feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
        "        pixel_values = pixel_values.to(self.device)\n",
        "\n",
        "        output_ids = self.model.generate(pixel_values, **self.gen_kwargs)\n",
        "\n",
        "        preds = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "        preds = [pred.strip() for pred in preds]\n",
        "        return preds\n",
        "\n",
        "    def streamlit_interface(self) -> None:\n",
        "\n",
        "        \"\"\"\n",
        "        Defines the Streamlit user interface and logic.\n",
        "        \"\"\"\n",
        "\n",
        "        st.title(\"Image Captioning App\")\n",
        "\n",
        "        # Add an upload button to let the user select an image\n",
        "        image_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "        if image_file is not None:\n",
        "            # Read the uploaded image\n",
        "            image = Image.open(image_file)\n",
        "\n",
        "            # Display the uploaded image\n",
        "            st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
        "\n",
        "            # Add a \"Generate Caption\" button to trigger the caption generation\n",
        "            if st.button(\"Generate \"):\n",
        "                # Call the predict_step function to generate a caption\n",
        "                caption = self._predict_step([image_file])[0]\n",
        "\n",
        "                # Display the generated caption\n",
        "                st.write(\"Generated Text :\", caption)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    img_to_txt = ImageToText()\n",
        "    img_to_txt.streamlit_interface()"
      ],
      "metadata": {
        "id": "oxcTmrRbyiY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "CaGs30Rvz6tg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}